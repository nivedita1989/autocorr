{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Petition Coreference entity sentiment analysis",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPu0srGBwQez2aNf8daQg8I",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nivedita1989/autocorr/blob/master/Petition_Coreference_entity_sentiment_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13sSN5ncwucU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "59104a32-8637-4288-87ae-d5bc5bcb94a3"
      },
      "source": [
        "'''#!python -m spacy download en_core_web_lg\n",
        "# NLTK\n",
        "#!pip install pycorenlp\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import wordnet, sentiwordnet\n",
        "# Spacy\n",
        "import spacy\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "nlp = spacy.load('en_core_web_lg') # For word and sentence level tokenization and POS tagging\n",
        "from pycorenlp import StanfordCoreNLP\n",
        "'''\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('sentiwordnet')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package sentiwordnet to /root/nltk_data...\n",
            "[nltk_data]   Package sentiwordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7eahiWtgES3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f=open('TestSentAna.txt','r')\n",
        "text=f.read()\n",
        "#text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCUkJ4La0Kq1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"Making NEGATORS word list\"\"\"\n",
        "NEGATORS = [\"ain't\",'cannot',\"can't\",\"didn't\",\"doesn't\",\"don't\",\"hadn't\",'hardly',\n",
        "            \"hasn't\",\"haven't\",\"havn't\",\"isn't\",'lack','lacking','lacks','neither',\n",
        "            'never','no','nobody','none','nor','not','nothing','nowhere',\"mightn't\",\n",
        "            \"mustn't\",\"needn't\",\"oughtn't\",\"shouldn't\",\"wasn't\",'without',\"wouldn't\",\n",
        "            'are not','can not', 'did not','does not','do not','had not', 'has not',\n",
        "            'have not','is not','might not','must not','need not','ought not','should not',\n",
        "            'was not','would not', \"n't\"]\n",
        "NEGATORS.sort(key = lambda x: 1/len(x))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-aEkQ5mV1Ick",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "87f71a9c-1103-47e6-e4e0-20fd8b67a175"
      },
      "source": [
        "\n",
        "f = open(\"TestSentAna.txt\", \"r\")\n",
        "text=f.read()\n",
        "def text_clean(text):\n",
        "  text = ' '+text+' '\n",
        "  for negator in NEGATORS:\n",
        "      text = text.replace(negator, '\"not\"')\n",
        "  text = re.sub(r'\\s+', ' ', text) # Remove repeated spaces/newlines\n",
        "  text = re.sub(r'(<\\^).*', '', text) # Remove anything after <^\n",
        "\n",
        "  text = re.sub(r'\\((.*?)\\)',' ', text) # Remove ('text')\n",
        "  text = re.sub(r'\\[(.*?)\\]',' ', text) # Remove ['text']\n",
        "  text = re.sub(r'<(.*?)>',' ', text) # Remove <'text'>\n",
        "  text = re.sub(r'\\S*@\\S*\\s?', ' ', text) # Remove emails\n",
        "  text = re.sub(r'http\\S+', ' ', text) # Remove URLs\n",
        "\n",
        "  text = re.sub(r'\\s+', ' ', text) # Remove repeated spaces/newlines again\n",
        "  text = text.replace(' *','.')\n",
        "  text = text.replace('..','.')\n",
        "  text = re.sub(r'\\s+', ' ', text) # Remove repeated spaces/newlines again\n",
        "  text = ' ' + text + ' ' # Add spaces surrounding the text, this makes finding whether or not a phrase occurs simpler\n",
        "  text = text.lower()\n",
        "\n",
        "  for c in ''':'.,\";-''':\n",
        "      text = text.replace(c, ' '+c+' ')\n",
        "  text = re.sub(r'''[^a-zA-Z0-9,.;:/\\-'\"]+''', ' ', text) # Get rid of special characters, except some punctuation\n",
        "\n",
        "  text = text.replace(\" won ' t \", \" won't \")\n",
        "  text = text.replace('n t', 'not')\n",
        "\n",
        "  return text\n",
        "  ###########\n",
        "newtext=text_clean(text)\n",
        "text=newtext\n",
        "print(text)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " petition made change with , rindani started petitionoto rajeev chandrasekhar and othersit seems like the popular ' beti padhao , beti bachao ' sloganothat we all love to chant does not cover all the young girls that need 13 . there is serious shortage of secondary and senior secondary schools for girls in 24 . data shows that in - , for every primary schools there are only secondary and senior secondary schools . shortage of schools ruins the future of lakhs of in 24 . 63 are forced to dropout and become child brides or teenage mothers , with a host of health problems . as a self - confident , educated woman who got to go to college , 44 want to ensure that every girl in 24 has a school to go to . sign 44 petition asking congress and the bjp to make this a priority in 50 election manifestos . moreschoolsforgirls when 65 re in a positionoto help someone less privileged than 65 , the first instinct for many of 65 is to ask two questions : if the person has children , and if children go to school . because 65 k \" not \" w that education is important . especially to break cycles of poverty and disadvantage . right to education act made 73 compulsory for children betweenothe age of and to get free education . but after years of age , which is usually class or , the dropout rates beginoto increase . especially for girls . aside from the \" \" \" not \" t \" \" of senior secondary and secondary schools , girls also dropout because schools do not have separate toilets for and girls and by the age of , most of 90 hit puberty . a \" \" not \" t \" her reason for higher dropout rates of girls is safety . all of these issues can be addressed by more schools for girls sign petitionoto make political parties contesting inothe general elections commit in 105 manifestos to increasing the number of secondary and senior secondary schools for girls in country . the rte act was a huge step forward in making sure girls got equal educational opportunities to boys . but when girls reach secondary education levels , dropout rate of girls increases to about and by class , 123 s around . the number of schools also reduce sharply beyond primary school . in - , for every schools offering classes to , only offered secondary level classes and and - even less - offered senior secondary level classes and . when 143 understand how important education is , how can 143 stand by and watch the future women leaders of 143 country fall by the wayside because of inadequate schools sign petition and together , 143 can convince 143 politicians that this is the time to prioritize the education of girls . in , whoever comes to power , school - age girls must win shevotes moreschoolsforgirls \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-NY_yZh3Si1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "83280067-a012-476f-b69c-869dbf80d230"
      },
      "source": [
        "sentences = [str(s).replace('.','') for s in nlp(text).sents]\n",
        "sentences\n",
        "num_chain_dic={'65': ['us', 'we', 'we', 'us'], '73': ['The Right to Education ( RTE ) Act', 'it'], '105': ['their', 'the political parties'], '137': ['the country', 'our country'], '44': ['I', 'my'], '13': ['it', 'supporters!Aditi Rindani'], '141': ['This petition', 'my petition', 'my petition', 'this petition'], '143': ['we', 'our', 'we', 'our', 'we'], '50': ['their', 'the Congress and the BJP'], '24': ['India', 'India', 'India'], '90': ['them', 'boys and girls'], '27': ['This shortage of schools', 'a serious shortage of secondary ( Class and ) and senior secondary schools ( Class'], '123': ['it', 'the dropout rate of girls'], '63': ['They', 'those children', 'girls in India']}\n",
        "entity_lis=num_chain_dic.keys()\n",
        "entity_lis"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['65', '73', '105', '137', '44', '13', '141', '143', '50', '24', '90', '27', '123', '63'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AK11bHhOUjjy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Sentence(object):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    def __init__(self, text):\n",
        "        self.raw = text\n",
        "        self.text = text\n",
        "        self.tokens = [token for token in nlp(self.text)] # word_tokenize(self.text)\n",
        "        self.pos_tags = list(zip(map(str, self.tokens), map(lambda token: token.pos_, self.tokens))) # nltk.pos_tag(self.tokens)\n",
        "        self.sent_tags = [(tup[0], word_to_sentiment(word=tup[0], pos=tup[1])) for tup in self.pos_tags]\n",
        "        self.pos_and_sent_tags = [(self.pos_tags[i][0], self.pos_tags[i][1], self.sent_tags[i][1]) for i in range(len(self.pos_tags))]\n",
        "        self.total_sent = list_to_score(self.pos_tags)\n",
        "        self.posneg_count = posneg_count(self.pos_and_sent_tags)\n",
        "\n",
        "    def parse(self):\n",
        "        \"\"\"\n",
        "        Splits a sentence into a list of chunks comprising the sentence (uses the pos_and_sent_tags attribute)\n",
        "        \"\"\"\n",
        "        # Find number of currencies in the sentence\n",
        "        if unique_entity_count(self.pos_and_sent_tags) == 1: # If this number is 1, return the entire sentence\n",
        "            return [self.pos_and_sent_tags]\n",
        "        elif unique_entity_count(self.pos_and_sent_tags) == 0: # If this number is 0, return an empty list\n",
        "            return []\n",
        "        if unique_entity_count(self.pos_and_sent_tags) > 1: # If this number is > 1, parse on commas\n",
        "            phrase_list = []\n",
        "            tags_so_far = []\n",
        "            for tup in self.pos_and_sent_tags:\n",
        "                token = tup[0]\n",
        "                if (token != ',') and (token != ';'):\n",
        "                    tags_so_far.append(tup)\n",
        "                elif (token == ',') or (token == ';'):\n",
        "                    phrase_list.append(tags_so_far)\n",
        "                    tags_so_far = []\n",
        "            phrase_list.append(tags_so_far)\n",
        "        if len(phrase_list) == 0:\n",
        "            return phrase_list\n",
        "\n",
        "        # Discard phrases with a unique_curr_count of 0\n",
        "        phrase_list = list(filter(lambda x: unique_entity_count(x) > 0, phrase_list))\n",
        "\n",
        "        def phrase_splitter(phrase):\n",
        "            if unique_entity_count(phrase) > 1 and posneg_count(phrase) > 1:\n",
        "                part1 = phrase[:1]\n",
        "                i = 0\n",
        "                while unique_entity_count(part1) == 0:\n",
        "                    part1 = phrase[:i]\n",
        "                    i += 1\n",
        "                while posneg_count(part1) == 0:\n",
        "                    part1 = phrase[:i]\n",
        "                    i += 1\n",
        "\n",
        "                part2 = phrase[i-1:]\n",
        "                if unique_entity_count(part2) == 0:\n",
        "                    return phrase\n",
        "                else:\n",
        "                    return [part1, part2]\n",
        "            else:\n",
        "                return [phrase]\n",
        "\n",
        "        split_phrase_list = []\n",
        "        for phrase in phrase_list:\n",
        "            split_phrase_list.extend(phrase_splitter(phrase))\n",
        "\n",
        "        return split_phrase_list\n",
        "\n",
        "    def sent_dic_wordnet(self):\n",
        "\n",
        "        chunks = self.parse()\n",
        "        sent_dict_list = []\n",
        "\n",
        "        for chunk in chunks:\n",
        "            sent_dict = dict()\n",
        "            sent = list_to_score(chunk)\n",
        "\n",
        "            # Check if a negator is in the chunk. if so, reverse score\n",
        "            text = ' '.join([tag[0] for tag in chunk])\n",
        "\n",
        "            if ' not ' in ' '+text+' ':\n",
        "                sent *= -0.9\n",
        "\n",
        "            # for word in NEGATORS:\n",
        "            #     if ' '+word+' ' in ' '+text+' ':\n",
        "            #         sent *= -0.9 # Literal is to diminish how much negation flips sentiment\n",
        "            #         break\n",
        "\n",
        "            for entity in unique_entities(chunk):\n",
        "                sent_dict[entity] = sent\n",
        "            sent_dict_list.append(sent_dict)\n",
        "\n",
        "        # Combine each dict in sent_dict_list, summing common entries\n",
        "        combined_dict = dict()\n",
        "        for entity in unique_entities(self.pos_and_sent_tags):\n",
        "            combined_dict[entity] = sum([dic.get(entity, 0) for dic in sent_dict_list])\n",
        "\n",
        "        return combined_dict\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SGw7P3NPzIi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "def find_sent_dic(text):\n",
        "    if type(text)!=str:\n",
        "        return dict()\n",
        "    S = Sentence(text)\n",
        "    return S.sent_dic_wordnet()\n",
        "    '''try:\n",
        "        return S.sent_dic_wordnet()\n",
        "    except:\n",
        "        return dict()'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_H9G5kbO_2A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "17638d42-e009-409b-9b71-95285f9bbe30"
      },
      "source": [
        "S=Sentence(sentences[7])\n",
        "print(sentences[7])\n",
        "#unique_entity_count(S.pos_and_sent_tags)\n",
        "S.sent_dic_wordnet()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "as a self - confident , educated woman who got to go to college , 44 want to ensure that every girl in 24 has a school to go to \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'24': 0.025, '44': -0.075}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2I52uVMKVuNE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def posneg_count(sent_tags):\n",
        "    \"\"\"\n",
        "    Number of either positive or negative words in a list of pos_and_sent_tags\n",
        "    \"\"\"\n",
        "    nonzero_sentiments = list(filter(lambda tup: tup[2] != 0, sent_tags))\n",
        "    return len(nonzero_sentiments)\n",
        "\n",
        "def unique_entities(pos_and_sent_tags):\n",
        "    text = ' '.join([tup[0] for tup in pos_and_sent_tags])\n",
        "    return set(filter(lambda c: c in text, entity_lis))\n",
        "\n",
        "def unique_entity_count(pos_and_sent_tags):\n",
        "    return len(unique_entities(pos_and_sent_tags))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRXJlOkMWICB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def word_to_sentiment(word, pos=None):\n",
        "    \"\"\"\n",
        "    Given a word and an optional part of speech tag,\n",
        "    returns the average sentiment score for that word using senti_synsets.\n",
        "    \"\"\"\n",
        "    if word in STOP_WORDS or word in NEGATORS:\n",
        "        return 0\n",
        "\n",
        "    if pos == None:\n",
        "        synsets = list(sentiwordnet.senti_synsets(word))\n",
        "\n",
        "    elif pos != None:\n",
        "        def get_wordnet_pos(tag):\n",
        "            \"\"\"\n",
        "            Translating Spacy's POS tags to WordNet's POS tags\n",
        "            \"\"\"\n",
        "            if tag.startswith('ADJ'): return wordnet.ADJ\n",
        "            elif tag.startswith('VERB'): return wordnet.VERB\n",
        "            elif tag.startswith('NOUN'): return wordnet.NOUN\n",
        "            elif tag.startswith('ADV'): return wordnet.ADV\n",
        "            else: return wordnet.NOUN\n",
        "\n",
        "        wordnet_pos = get_wordnet_pos(pos)\n",
        "        synsets = list(sentiwordnet.senti_synsets(word, wordnet_pos))\n",
        "\n",
        "    score = 0\n",
        "    for el in synsets:\n",
        "        score += el.pos_score()\n",
        "        score -= el.neg_score()\n",
        "\n",
        "    if score == 0 and pos != None:\n",
        "        return word_to_sentiment(word, pos=None)\n",
        "\n",
        "    if len(synsets) == 0: return 0\n",
        "\n",
        "    score /= len(synsets)\n",
        "    return score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHs0aJr4WnNj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def list_to_score(pos_tags):\n",
        "    \"\"\"\n",
        "    Finds the average sentiment of a list of tuples of (word, pos) using word_to_sentiment()\n",
        "    \"\"\"\n",
        "    score = 0\n",
        "    entity_count = 0\n",
        "    words_with_sentiment = 0\n",
        "    for tup in pos_tags:\n",
        "        word, pos = tup[0], tup[1]\n",
        "        if word in entity_lis:\n",
        "            entity_count += 1\n",
        "        else:\n",
        "            if word_to_sentiment(word, pos) != 0:\n",
        "                words_with_sentiment += 1\n",
        "                score += word_to_sentiment(word, pos)\n",
        "    if score == 0:\n",
        "        return 0\n",
        "    else:\n",
        "        return score/words_with_sentiment"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sodV9IuUhONl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "fd965a2c-0521-4dae-ba1f-d1ecb67bce41"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package sentiwordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/sentiwordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPtd_1Y7WrJx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences = [str(s).replace('.','') for s in nlp(text).sents]\n",
        "\n",
        "sent_dict_list = [find_sent_dic(s) for s in sentences]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPQh_W-VgOlg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Combine dictionaries, summing common entries:\n",
        "combined_dict = dict()\n",
        "for entity in entity_lis:\n",
        "    combined_dict[entity] = sum([dic.get(entity, 0) for dic in sent_dict_list])\n",
        "combined_dict = {k:v for k,v in combined_dict.items() if v != 0}\n",
        "\n",
        "sign = lambda x: 'Negative' if x < -.01 else 'Positive' if x > .01 else 'Neutral'\n",
        "combined_dict = {k:sign(v) for k,v in combined_dict.items()}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzF0EKOnhdrH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "53e512bb-c64f-4281-e1d4-52926615e1b4"
      },
      "source": [
        "combined_dict\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'105': 'Negative',\n",
              " '13': 'Negative',\n",
              " '143': 'Positive',\n",
              " '24': 'Negative',\n",
              " '44': 'Negative',\n",
              " '50': 'Positive',\n",
              " '63': 'Positive',\n",
              " '65': 'Negative',\n",
              " '73': 'Positive',\n",
              " '90': 'Positive'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "foPOMYHFhgA-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "7a77f780-999d-4ab0-9bf8-a6bcfebee195"
      },
      "source": [
        "num_chain_dic"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'105': ['their', 'the political parties'],\n",
              " '123': ['it', 'the dropout rate of girls'],\n",
              " '13': ['it', 'supporters!Aditi Rindani'],\n",
              " '137': ['the country', 'our country'],\n",
              " '141': ['This petition', 'my petition', 'my petition', 'this petition'],\n",
              " '143': ['we', 'our', 'we', 'our', 'we'],\n",
              " '24': ['India', 'India', 'India'],\n",
              " '27': ['This shortage of schools',\n",
              "  'a serious shortage of secondary ( Class and ) and senior secondary schools ( Class'],\n",
              " '44': ['I', 'my'],\n",
              " '50': ['their', 'the Congress and the BJP'],\n",
              " '63': ['They', 'those children', 'girls in India'],\n",
              " '65': ['us', 'we', 'we', 'us'],\n",
              " '73': ['The Right to Education ( RTE ) Act', 'it'],\n",
              " '90': ['them', 'boys and girls']}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQ4xL7MHuKvX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}